"""
M&Aäºˆå…†æ¤œçŸ¥ã‚¨ãƒ³ã‚¸ãƒ³
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œçŸ¥
- M&Aã‚¹ã‚³ã‚¢ç®—å‡º
"""
from __future__ import annotations
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import re
import time
import random
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pandas as pd

# ==========================================
# è¨­å®š
# ==========================================
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# M&Aé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé‡è¦åº¦é †ï¼‰
MA_KEYWORDS = {
    # æœ€é‡è¦ï¼ˆç›´æ¥çš„ãªM&Aã‚·ã‚°ãƒŠãƒ«ï¼‰
    "critical": [
        "å®Œå…¨å­ä¼šç¤¾åŒ–", "TOB", "æ ªå¼å…¬é–‹è²·ä»˜", "MBO", "æ ªå¼äº¤æ›",
        "å¸ååˆä½µ", "çµŒå–¶çµ±åˆ", "è²·å", "å­ä¼šç¤¾åŒ–", "è¦ªä¼šç¤¾",
        "æ ªå¼ç§»è»¢", "ã‚¹ã‚¯ã‚¤ãƒ¼ã‚ºã‚¢ã‚¦ãƒˆ", "å°‘æ•°æ ªä¸»", "ä¸Šå ´å»ƒæ­¢"
    ],
    # é‡è¦ï¼ˆé–“æ¥çš„ãªã‚·ã‚°ãƒŠãƒ«ï¼‰
    "high": [
        "è³‡æœ¬ææº", "æ¥­å‹™ææº", "ç¬¬ä¸‰è€…å‰²å½“", "å¤§æ ªä¸»", "ç­†é ­æ ªä¸»",
        "æ ªå¼å–å¾—", "æŒæ ªæ¯”ç‡", "æ”¯é…æ¨©", "çµŒå–¶æ¨©", "äº‹æ¥­è­²æ¸¡",
        "å†ç·¨", "ãƒªã‚¹ãƒˆãƒ©", "æ§‹é€ æ”¹é©", "å†…è£½åŒ–", "ã‚°ãƒ«ãƒ¼ãƒ—å†ç·¨"
    ],
    # å‚è€ƒï¼ˆæ³¨æ„ã™ã¹ãã‚·ã‚°ãƒŠãƒ«ï¼‰
    "medium": [
        "ã‚·ãƒŠã‚¸ãƒ¼", "ç›¸ä¹—åŠ¹æœ", "äº‹æ¥­çµ±åˆ", "åŠ¹ç‡åŒ–", "ã‚³ã‚¹ãƒˆå‰Šæ¸›",
        "åç›Šæ”¹å–„", "é»’å­—åŒ–", "å¢—é…", "è‡ªç¤¾æ ªè²·ã„", "æ ªä¸»é‚„å…ƒ",
        "ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ã‚¹ãƒˆ", "ç‰©è¨€ã†æ ªä¸»", "æ ªä¸»ææ¡ˆ", "æ•µå¯¾çš„"
    ]
}

# é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆM&Aé˜»å®³è¦å› ï¼‰
EXCLUSION_KEYWORDS = [
    "è‡ªç¤¾æ ªè²·ã„ç™ºè¡¨", "å¤§è¦æ¨¡è‡ªç¤¾æ ªè²·ã„", "è²·åé˜²è¡›ç­–", "ãƒã‚¤ã‚ºãƒ³ãƒ”ãƒ«"
]


class MASignalLevel(Enum):
    """M&Aã‚·ã‚°ãƒŠãƒ«ãƒ¬ãƒ™ãƒ«"""
    CRITICAL = "ğŸ”´ ç·Šæ€¥"
    HIGH = "ğŸŸ  é«˜"
    MEDIUM = "ğŸŸ¡ ä¸­"
    LOW = "ğŸŸ¢ ä½"
    NONE = "âšª ãªã—"


@dataclass
class NewsItem:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ """
    title: str
    url: str
    source: str
    date: Optional[datetime] = None
    snippet: str = ""
    matched_keywords: List[str] = field(default_factory=list)
    signal_level: MASignalLevel = MASignalLevel.NONE


@dataclass
class MAScore:
    """M&Aäºˆå…†ã‚¹ã‚³ã‚¢"""
    code: str
    name: str
    total_score: int  # 0-100
    signal_level: MASignalLevel
    news_score: int  # ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æã‚¹ã‚³ã‚¢ï¼ˆ0-40ï¼‰
    volume_score: int  # å‡ºæ¥é«˜ç•°å¸¸ã‚¹ã‚³ã‚¢ï¼ˆ0-30ï¼‰
    valuation_score: int  # ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆ0-20ï¼‰
    technical_score: int  # ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚¹ã‚³ã‚¢ï¼ˆ0-10ï¼‰
    news_items: List[NewsItem] = field(default_factory=list)
    matched_keywords: List[str] = field(default_factory=list)
    exclusion_flags: List[str] = field(default_factory=list)
    reason_tags: List[str] = field(default_factory=list)


def get_sleep_time() -> float:
    """ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¹ãƒªãƒ¼ãƒ—æ™‚é–“ã‚’è¿”ã™"""
    return random.uniform(1.0, 2.5)


def scrape_yahoo_news(query: str, max_results: int = 10) -> List[NewsItem]:
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    news_items = []
    
    try:
        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢
        search_url = f"https://news.yahoo.co.jp/search?p={requests.utils.quote(query)}&ei=UTF-8"
        
        time.sleep(get_sleep_time())
        response = requests.get(search_url, headers=HEADERS, timeout=10)
        
        if response.status_code != 200:
            return news_items
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æŠ½å‡º
        articles = soup.select('div.newsFeed_item, article.newsFeed_item, div[class*="NewsItem"]')
        
        if not articles:
            # åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
            articles = soup.select('a[href*="/articles/"]')
        
        for article in articles[:max_results]:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                title_elem = article.select_one('h2, h3, span[class*="title"], div[class*="title"]')
                if not title_elem:
                    title_elem = article
                title = title_elem.get_text(strip=True)
                
                if not title or len(title) < 5:
                    continue
                
                # URLå–å¾—
                link = article.select_one('a[href*="/articles/"]')
                if link:
                    url = link.get('href', '')
                    if not url.startswith('http'):
                        url = f"https://news.yahoo.co.jp{url}"
                else:
                    url = ""
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                matched = []
                signal = MASignalLevel.NONE
                
                for kw in MA_KEYWORDS["critical"]:
                    if kw in title:
                        matched.append(kw)
                        signal = MASignalLevel.CRITICAL
                
                if signal == MASignalLevel.NONE:
                    for kw in MA_KEYWORDS["high"]:
                        if kw in title:
                            matched.append(kw)
                            signal = MASignalLevel.HIGH
                
                if signal == MASignalLevel.NONE:
                    for kw in MA_KEYWORDS["medium"]:
                        if kw in title:
                            matched.append(kw)
                            signal = MASignalLevel.MEDIUM
                
                news_items.append(NewsItem(
                    title=title,
                    url=url,
                    source="Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                    matched_keywords=matched,
                    signal_level=signal
                ))
                
            except Exception:
                continue
                
    except Exception as e:
        print(f"News scraping error: {e}")
    
    return news_items


def scrape_google_news(query: str, max_results: int = 5) -> List[NewsItem]:
    """
    Google Newsï¼ˆæ—¥æœ¬èªï¼‰ã‹ã‚‰é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    â€»ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«æ³¨æ„
    """
    news_items = []
    
    try:
        search_url = f"https://news.google.com/search?q={requests.utils.quote(query)}&hl=ja&gl=JP&ceid=JP:ja"
        
        time.sleep(get_sleep_time())
        response = requests.get(search_url, headers=HEADERS, timeout=10)
        
        if response.status_code != 200:
            return news_items
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Google Newsã®è¨˜äº‹ã‚’æŠ½å‡º
        articles = soup.select('article')
        
        for article in articles[:max_results]:
            try:
                title_elem = article.select_one('h3, h4, a[href*="./articles/"]')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                if not title or len(title) < 5:
                    continue
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                matched = []
                signal = MASignalLevel.NONE
                
                for kw in MA_KEYWORDS["critical"]:
                    if kw in title:
                        matched.append(kw)
                        signal = MASignalLevel.CRITICAL
                
                if signal == MASignalLevel.NONE:
                    for kw in MA_KEYWORDS["high"]:
                        if kw in title:
                            matched.append(kw)
                            signal = MASignalLevel.HIGH
                
                news_items.append(NewsItem(
                    title=title,
                    url="",
                    source="Google News",
                    matched_keywords=matched,
                    signal_level=signal
                ))
                
            except Exception:
                continue
                
    except Exception:
        pass
    
    return news_items


def analyze_news_for_ma(company_name: str, code: str) -> Tuple[int, List[NewsItem], List[str]]:
    """
    ä¼æ¥­ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åˆ†æã—ã¦M&Aã‚¹ã‚³ã‚¢ã‚’ç®—å‡º
    
    Returns:
        (score, news_items, matched_keywords)
    """
    all_news = []
    all_keywords = set()
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å®Ÿè¡Œ
    queries = [
        f"{company_name} M&A",
        f"{company_name} TOB",
        f"{company_name} å®Œå…¨å­ä¼šç¤¾åŒ–",
        f"{code} æ ªå¼",
    ]
    
    for query in queries:
        news = scrape_yahoo_news(query, max_results=5)
        for item in news:
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯
            if not any(n.title == item.title for n in all_news):
                all_news.append(item)
                all_keywords.update(item.matched_keywords)
    
    # ã‚¹ã‚³ã‚¢ç®—å‡ºï¼ˆæœ€å¤§40ç‚¹ï¼‰
    score = 0
    
    # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°å¤§å¹…åŠ ç‚¹
    critical_count = sum(1 for n in all_news if n.signal_level == MASignalLevel.CRITICAL)
    high_count = sum(1 for n in all_news if n.signal_level == MASignalLevel.HIGH)
    medium_count = sum(1 for n in all_news if n.signal_level == MASignalLevel.MEDIUM)
    
    score += min(25, critical_count * 10)  # æœ€å¤§25ç‚¹
    score += min(10, high_count * 3)  # æœ€å¤§10ç‚¹
    score += min(5, medium_count * 1)  # æœ€å¤§5ç‚¹
    
    return min(40, score), all_news, list(all_keywords)


def calculate_volume_score(
    volume_ratio: Optional[float],
    turnover_pct: Optional[float],
    turnover_5d_pct: Optional[float]
) -> int:
    """
    å‡ºæ¥é«˜é–¢é€£ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆæœ€å¤§30ç‚¹ï¼‰
    """
    score = 0
    
    # å‡ºæ¥é«˜å€ç‡ï¼ˆå¯¾20æ—¥å¹³å‡ï¼‰
    if volume_ratio:
        if volume_ratio >= 5.0:
            score += 15  # ç•°å¸¸ãªæ€¥å¢—
        elif volume_ratio >= 3.0:
            score += 10
        elif volume_ratio >= 2.0:
            score += 5
    
    # å½“æ—¥å›è»¢ç‡
    if turnover_pct:
        if turnover_pct >= 10.0:
            score += 10  # æµ®å‹•æ ªã®10%ä»¥ä¸ŠãŒå›è»¢
        elif turnover_pct >= 5.0:
            score += 7
        elif turnover_pct >= 3.0:
            score += 3
    
    # 5æ—¥ç´¯ç©å›è»¢ç‡
    if turnover_5d_pct:
        if turnover_5d_pct >= 30.0:
            score += 5  # 1é€±é–“ã§æµ®å‹•æ ªã®3å‰²å…¥æ›¿
        elif turnover_5d_pct >= 15.0:
            score += 3
    
    return min(30, score)


def calculate_valuation_score(
    pbr: Optional[float],
    upside_pct: Optional[float],
    market_cap: Optional[float]
) -> int:
    """
    ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆæœ€å¤§20ç‚¹ï¼‰
    M&Aã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ãªã‚Šã‚„ã™ã„æ¡ä»¶ã‚’è©•ä¾¡
    """
    score = 0
    
    # PBRï¼ˆä½ã„ã»ã©è²·åãƒ¡ãƒªãƒƒãƒˆå¤§ï¼‰
    if pbr is not None:
        if pbr < 0.5:
            score += 8  # è¶…å‰²å®‰
        elif pbr < 0.8:
            score += 6
        elif pbr < 1.0:
            score += 4
    
    # æ™‚ä¾¡ç·é¡ï¼ˆä¸­å°å‹ãŒç‹™ã‚ã‚Œã‚„ã™ã„ï¼‰
    if market_cap:
        mc_oku = market_cap / 100000000  # å„„å††æ›ç®—
        if 300 <= mc_oku <= 2000:
            score += 6  # TOBã—ã‚„ã™ã„ã‚µã‚¤ã‚º
        elif 2000 < mc_oku <= 5000:
            score += 3
    
    # ç†è«–æ ªä¾¡ã¨ã®ä¹–é›¢ï¼ˆå‰²å®‰åº¦ï¼‰
    if upside_pct:
        if upside_pct >= 50:
            score += 6  # å¤§å¹…å‰²å®‰
        elif upside_pct >= 30:
            score += 4
        elif upside_pct >= 15:
            score += 2
    
    return min(20, score)


def calculate_technical_score(signal_icon: str) -> int:
    """
    ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºï¼ˆæœ€å¤§10ç‚¹ï¼‰
    """
    score_map = {
        "â†‘â—": 10,  # æ¿€ç†±
        "â†—ã€‡": 7,   # è²·ã„
        "â†’â–³": 3,   # æ§˜å­è¦‹
        "â†˜â–²": 1,   # å£²ã‚Š
        "â†“âœ–": 0,   # å±é™º
    }
    return score_map.get(signal_icon, 0)


def check_exclusion_factors(news_items: List[NewsItem]) -> Tuple[int, List[str]]:
    """
    M&Aé˜»å®³è¦å› ã‚’ãƒã‚§ãƒƒã‚¯
    Returns:
        (æ¸›ç‚¹ã‚¹ã‚³ã‚¢, æ¤œå‡ºã•ã‚ŒãŸãƒ•ãƒ©ã‚°)
    """
    penalty = 0
    flags = []
    
    for news in news_items:
        for kw in EXCLUSION_KEYWORDS:
            if kw in news.title:
                flags.append(kw)
                penalty += 15  # å¤§å¹…æ¸›ç‚¹
    
    return penalty, list(set(flags))


def generate_reason_tags(
    news_score: int,
    volume_score: int,
    valuation_score: int,
    matched_keywords: List[str]
) -> List[str]:
    """
    ã‚¹ã‚³ã‚¢ã®ç†ç”±ã‚¿ã‚°ã‚’ç”Ÿæˆ
    """
    tags = []
    
    if news_score >= 20:
        tags.append("ğŸ“° M&Aãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œçŸ¥")
    if volume_score >= 15:
        tags.append("ğŸ“ˆ å‡ºæ¥é«˜æ€¥å¢—")
    if valuation_score >= 12:
        tags.append("ğŸ’° å‰²å®‰Ã—è²·åé©æ­£ã‚µã‚¤ã‚º")
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚°
    if any(kw in matched_keywords for kw in ["å®Œå…¨å­ä¼šç¤¾åŒ–", "TOB", "æ ªå¼å…¬é–‹è²·ä»˜"]):
        tags.append("ğŸ¯ ç›´æ¥ã‚·ã‚°ãƒŠãƒ«")
    if any(kw in matched_keywords for kw in ["è¦ªä¼šç¤¾", "ã‚°ãƒ«ãƒ¼ãƒ—å†ç·¨", "å†…è£½åŒ–"]):
        tags.append("ğŸ¢ è¦ªå­é–¢ä¿‚")
    if any(kw in matched_keywords for kw in ["ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ã‚¹ãƒˆ", "ç‰©è¨€ã†æ ªä¸»"]):
        tags.append("ğŸ¦… ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ã‚¹ãƒˆ")
    
    return tags


def get_signal_level(total_score: int) -> MASignalLevel:
    """
    ç·åˆã‚¹ã‚³ã‚¢ã‹ã‚‰ã‚·ã‚°ãƒŠãƒ«ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š
    """
    if total_score >= 70:
        return MASignalLevel.CRITICAL
    elif total_score >= 50:
        return MASignalLevel.HIGH
    elif total_score >= 30:
        return MASignalLevel.MEDIUM
    elif total_score >= 15:
        return MASignalLevel.LOW
    else:
        return MASignalLevel.NONE


def analyze_ma_potential(
    code: str,
    name: str,
    price: Optional[float],
    pbr: Optional[float],
    upside_pct: Optional[float],
    market_cap: Optional[float],
    volume_ratio: Optional[float],
    turnover_pct: Optional[float],
    turnover_5d_pct: Optional[float],
    signal_icon: str,
    skip_news: bool = False
) -> MAScore:
    """
    M&Aäºˆå…†ã‚’ç·åˆåˆ†æ
    
    Args:
        skip_news: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã‹ï¼ˆé«˜é€ŸåŒ–ç”¨ï¼‰
    """
    # 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æï¼ˆæœ€å¤§40ç‚¹ï¼‰
    if skip_news:
        news_score = 0
        news_items = []
        matched_keywords = []
    else:
        news_score, news_items, matched_keywords = analyze_news_for_ma(name, code)
    
    # 2. å‡ºæ¥é«˜ã‚¹ã‚³ã‚¢ï¼ˆæœ€å¤§30ç‚¹ï¼‰
    volume_score = calculate_volume_score(volume_ratio, turnover_pct, turnover_5d_pct)
    
    # 3. ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆæœ€å¤§20ç‚¹ï¼‰
    valuation_score = calculate_valuation_score(pbr, upside_pct, market_cap)
    
    # 4. ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ã‚¹ã‚³ã‚¢ï¼ˆæœ€å¤§10ç‚¹ï¼‰
    technical_score = calculate_technical_score(signal_icon)
    
    # 5. é™¤å¤–è¦å› ãƒã‚§ãƒƒã‚¯
    penalty, exclusion_flags = check_exclusion_factors(news_items)
    
    # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
    raw_score = news_score + volume_score + valuation_score + technical_score
    total_score = max(0, raw_score - penalty)
    
    # ã‚·ã‚°ãƒŠãƒ«ãƒ¬ãƒ™ãƒ«åˆ¤å®š
    signal_level = get_signal_level(total_score)
    
    # ç†ç”±ã‚¿ã‚°ç”Ÿæˆ
    reason_tags = generate_reason_tags(news_score, volume_score, valuation_score, matched_keywords)
    
    return MAScore(
        code=code,
        name=name,
        total_score=total_score,
        signal_level=signal_level,
        news_score=news_score,
        volume_score=volume_score,
        valuation_score=valuation_score,
        technical_score=technical_score,
        news_items=news_items,
        matched_keywords=matched_keywords,
        exclusion_flags=exclusion_flags,
        reason_tags=reason_tags
    )


def batch_analyze_ma(
    stock_data_list: List[Dict[str, Any]],
    with_news: bool = True
) -> List[MAScore]:
    """
    è¤‡æ•°éŠ˜æŸ„ã‚’ä¸€æ‹¬ã§M&Aåˆ†æ
    """
    results = []
    
    for data in stock_data_list:
        if data.get("name") == "å­˜åœ¨ã—ãªã„éŠ˜æŸ„":
            continue
        
        # PBRè¨ˆç®—
        price = data.get("price")
        bps = None  # yfinanceã‹ã‚‰ç›´æ¥å–å¾—ã§ããªã„ãŸã‚ã€åˆ¥é€”è¨ˆç®—ãŒå¿…è¦
        
        # fair_value_calc_y4ã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        pbr = None
        if price and data.get("fair_value"):
            # ç°¡æ˜“çš„ã«PBRæ¨å®šï¼ˆç†è«–æ ªä¾¡ã‹ã‚‰é€†ç®—ï¼‰
            # Grahamæ•°: âˆš(22.5 Ã— EPS Ã— BPS) = fair_value
            # ã“ã®æƒ…å ±ã ã‘ã§ã¯PBRã¯ç®—å‡ºã§ããªã„ãŒã€å¸‚å ´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ä½¿ç”¨
            pass
        
        score = analyze_ma_potential(
            code=data.get("code", ""),
            name=data.get("name", ""),
            price=price,
            pbr=pbr,
            upside_pct=data.get("upside_pct"),
            market_cap=data.get("market_cap"),
            volume_ratio=data.get("volume_ratio"),
            turnover_pct=data.get("turnover_pct"),
            turnover_5d_pct=data.get("turnover_5d_pct") if "turnover_5d_pct" in data else None,
            signal_icon=data.get("signal_icon", "â€”"),
            skip_news=not with_news
        )
        
        results.append(score)
    
    # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x.total_score, reverse=True)
    
    return results
